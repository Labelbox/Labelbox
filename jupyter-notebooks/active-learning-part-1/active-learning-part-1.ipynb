{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Active Learning With Labelbox (Part 1): Uncertainty Sampling using Queue Customization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine learning as applied in non-academic settings is often an incredibly iterative process where the majority of time is spent managing and curating training data.  As accurately outlined by Andrej Karpathy:\n",
    "\n",
    "![](./images/dataset.jpg)\n",
    "\n",
    "Machine learning engineers spend most of time curating datasets for their models rather than tweak the models directly.  Engineers have to wade through large (often unbounded) unlabeled datasets while adhering to a budget for annotations and training cycles.\n",
    "\n",
    "\n",
    "The solution is to rely on an [active learning framework](http://burrsettles.com/pub/settles.activelearning.pdf) to surface the most informative examples to minimize annotation and training cycle constraints.\n",
    "\n",
    "![taken from https://lexfridman.com/files/slides/2020_01_06_deep_learning_state_of_the_art.pdf](./images/data_engine.png)\n",
    "\n",
    "\n",
    "A typical workflow looks like:\n",
    "1. Start with a small portion of the dataset labeled\n",
    "2. Train a classifier on our labeled dataset\n",
    "3. Prioritize a new batch of the dataset for labeling\n",
    "4. Retrain our classifier with the expanded labeled dataset\n",
    "5. Repeat\n",
    "\n",
    "The simplest method for choosing which datarows to prioritize is **uncertainty sampling**.  Under this paradigm, practicioners will define an uncertainty metric to capture datarows for which a model is least confident in its decision and prioritize those examples for annotation and training.\n",
    "\n",
    "**In Part 1 of this series, we will use Labelbox to implement a simple active learning framework on a relatively easy problem and dataset.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Software dependencies\n",
    "This notebook can be run using the following [Dockerfile](./Dockerfile) and [requirements.txt](./requirements.txt)\n",
    "\n",
    "Build the Docker image locally:\n",
    "\n",
    "```\n",
    "docker build -t active-learning-part-1 .\n",
    "```\n",
    "\n",
    "Launch the image with access to this notebook from within the current directory:\n",
    "```\n",
    "docker run -it -p 8888:8888 -v \\\n",
    "${PWD}:/usr/src/app -w /usr/src/app active-learning-part-1 \\\n",
    "jupyter notebook --ip 0.0.0.0 --no-browser  --allow-root\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset details\n",
    "\n",
    "Sentiment analysis on tweets taken from Twitter is a well-explored and tractable nlp problem.  We take a random 8,000 tweets from the [`sentiment140`](https://www.kaggle.com/kazanova/sentiment140) dataset that are already classified as positive (`0`) or negative (`4`) sentiment.\n",
    "\n",
    "The original [csv with 16 million tweets](./data/training.1600000.processed.noemoticon.csv), [shuffled csv](./data/training.1600000.processed.noemoticon.csv.shuffled), and [8000 random tweets csv](./data/training_8000_shuffled.csv) are all included for reference.\n",
    "\n",
    "The derived csv files were created by sorting randomly and then taking the first 8000 rows:\n",
    "\n",
    "```\n",
    "$sort -R training.1600000.processed.noemoticon.csv > training.1600000.processed.noemoticon.csv.shuffled\n",
    "$head -8000 training.1600000.processed.noemoticon.csv.shuffled > training_8000_shuffled.csv\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Ingest into Labelbox\n",
    "In the real world, we would first label a subset of our data with Labelbox, to do so, we need to:\n",
    "1. [Create a project](https://labelbox.com/docs/python-api/create-first-project)\n",
    "2. Create and upload our dataset\n",
    "3. [Configure the Labelbox text classification editor](https://labelbox.com/docs/nlp/text-classification)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collapse\n",
    "from collections import defaultdict\n",
    "from enum import Enum\n",
    "from itertools import chain, islice\n",
    "import json\n",
    "import os\n",
    "from typing import Callable, Dict, List, Tuple\n",
    "\n",
    "from labelbox import Client, Dataset, LabelingFrontend, Project\n",
    "from labelbox import schema\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.legend import Legend\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from graphqlclient import GraphQLClient\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "import subprocess\n",
    "from transformers import DistilBertModel, DistilBertTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by loading our 8000 tweets into a DataFrame, taking 800 for our validation set and 7200 for our training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collapse\n",
    "RANDOM_SEED = 123\n",
    "DATA_DIR = 'data'\n",
    "\n",
    "def load_non_weather_data():\n",
    "    data = pd.read_csv(\n",
    "        filepath_or_buffer=os.path.join(DATA_DIR, 'training_8000_shuffled.csv'),\n",
    "        names=[\"target\", \"ids\", \"date\", \"flag\", \"user\", \"text\"],\n",
    "        encoding='ISO-8859-1'\n",
    "    )\n",
    "    data.rename(columns=\n",
    "        {\n",
    "            'target': 'sentiment',\n",
    "            'ids': 'id'\n",
    "        }, inplace=True\n",
    "    )\n",
    "    # 0 is positive\n",
    "    # 4 is negative in the original dataset, use 1 for clarity\n",
    "    data.sentiment = data.sentiment.map({0: 0, 4: 1}).values\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "def train_val_split(df, test_frac: float):\n",
    "    test_rows = df.sample(frac=test_frac, random_state=RANDOM_SEED)\n",
    "    df['split'] = df.id.apply(lambda row: 'test' if row in test_rows.id.values else 'train')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SENTIMENT_DF = load_non_weather_data()\n",
    "SENTIMENT_DF = train_val_split(SENTIMENT_DF, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collapse\n",
    "# labelbox setup names\n",
    "PROJECT_NAME = 'tweet_sentiment_demo'\n",
    "DATASET_NAME = 'tweet_sentiment_demo'\n",
    "LABELING_FRONTEND_NAME = \"Binary sentiment editor\"\n",
    "\n",
    "\n",
    "def maybe_create_project(project_name: str) -> schema.project.Project:\n",
    "    '''Creates project if it does not already exist, otherwise fetches.\n",
    "    \n",
    "    NOTE: we assume there is only one project with a given\n",
    "    project name.  this is not gauranteed, but sufficient\n",
    "    for a tutorial.\n",
    "    \n",
    "    '''\n",
    "    client = Client(LABELBOX_API_KEY)\n",
    "    projects = client.get_projects(where=Project.name == project_name)\n",
    "    try:\n",
    "        project = next(iter(projects))\n",
    "    except StopIteration:\n",
    "        project = client.create_project(name=project_name)\n",
    "    return project\n",
    "\n",
    "\n",
    "def maybe_create_sentiment_dataset(dataset_name: str, project_name: str) -> schema.dataset.Dataset:\n",
    "    '''Creates our sentiment dataset with 80k datarows if it does not already exist, otherwise fetches.\n",
    "    '''\n",
    "    \n",
    "    client = Client(LABELBOX_API_KEY)\n",
    "    project = maybe_create_project(project_name)\n",
    "    datasets = client.get_datasets(where=Dataset.name == DATASET_NAME)\n",
    "    try:\n",
    "        dataset = next(iter(datasets))\n",
    "    except StopIteration:\n",
    "        dataset = client.create_dataset(name=DATASET_NAME, projects=project)\n",
    "\n",
    "    try:\n",
    "        next(iter(dataset.data_rows()))\n",
    "    except StopIteration:\n",
    "        task = dataset.create_data_rows(sentiment_df_to_json_rows())\n",
    "        task.wait_till_done()\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def update_with_uid(df, project_name: str, dataset_name: str) -> pd.core.frame.DataFrame:\n",
    "    \"\"\"Add uid column for tracking labelbox's id of the same datarow in our dataframe.\n",
    "    \n",
    "    Args:\n",
    "        df: dataframe to augment\n",
    "        project_name: project name for dataset\n",
    "        dataset_name: name for dataset\n",
    "        \n",
    "    \"\"\"\n",
    "    project = maybe_create_project(project_name)\n",
    "    dataset = maybe_create_sentiment_dataset(dataset_name, project_name)\n",
    "    \n",
    "    external_uid_map = {\n",
    "        int(data_row.external_id): data_row.uid\n",
    "        for data_row in dataset.data_rows()\n",
    "    }\n",
    "    df['uid'] = df.id.map(external_uid_map)\n",
    "    return df\n",
    "\n",
    "\n",
    "def graphql_client() -> GraphQLClient:\n",
    "    '''Auths/returns client for interacting with LabelBox's GraphQL API.'''\n",
    "    client = GraphQLClient('https://api.labelbox.com/graphql')\n",
    "    client.inject_token(f'Bearer {LABELBOX_API_KEY}')\n",
    "    return client\n",
    "\n",
    "\n",
    "def create_labeling_frontend() -> str:\n",
    "    '''Creates a labeling frontend and returns its id.'''\n",
    "    client = graphql_client()\n",
    "    response = client.execute('''\n",
    "        mutation CreateLabelingFrontend {\n",
    "          createLabelingFrontend(data: {\n",
    "            description: \"Classify sentiment as positive or negative\",\n",
    "            iframe_url_path: \"https://classification.labelbox.com\",\n",
    "            name: $name\n",
    "          }){\n",
    "            id\n",
    "          }\n",
    "        }\n",
    "    ''', {'name': LABELING_FRONTEND_NAME})\n",
    "    return json.loads(response)['data']['CreateLabelingFrontend']['id']\n",
    "\n",
    "    \n",
    "\n",
    "def maybe_create_ontology(project_name: str) -> None:\n",
    "    '''Creates/grabs ontology for sentiment annotation.\n",
    "    \n",
    "    1. Sets up ontology for binary sentiment classification.\n",
    "    2. Sets labeling frontend using the classification iframe.\n",
    "    3. Links these with the given project.\n",
    "    \n",
    "    '''\n",
    "    client = Client(LABELBOX_API_KEY)\n",
    "    project = maybe_create_project(project_name)\n",
    "    ontology = '''\n",
    "        {\n",
    "            \"tools\": [],\n",
    "            \"classifications\": [\n",
    "                {\n",
    "                    \"name\": \"Sentiment\",\n",
    "                    \"instructions\": \"Is this tweet primarily positive or negative in sentiment?\",\n",
    "                    \"type\": \"radio\",\n",
    "                    \"options\": [\n",
    "                        {\n",
    "                            \"value\": 0,\n",
    "                            \"label\": \"Negative\"\n",
    "                        },\n",
    "                        {\n",
    "                            \"value\": 1,\n",
    "                            \"label\": \"Positive\"\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    '''\n",
    "    frontends = client.get_labeling_frontends(where=LabelingFrontend.name == LABELING_FRONTEND_NAME)\n",
    "    try:\n",
    "        frontend = next(iter(frontends))\n",
    "    except StopIteration:\n",
    "        frontend_id = create_labeling_frontend()\n",
    "        frontends = client.get_labeling_frontends(\n",
    "            where=LabelingFrontend.id == frontend_id)\n",
    "        frontend = next(iter(frontends))\n",
    "    project.setup(frontend, ontology)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a project named `PROJECT_NAME` with a dataset named `DATASET_NAME` using our API key, `LABELBOX_API_KEY`.  After uploading our 8000 tweets to Labelbox, we update our DataFrame with the [DataRow.uid](https://labelbox.com/docs/python-api/api-reference#data_row) to keep track of uid's given by Labelbox.  To finish project setup, we link it with an labeling frontend and ontology specifically for [Text Classification](https://labelbox.com/docs/nlp/text-classification)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = maybe_create_sentiment_dataset(dataset_name=DATASET_NAME, project_name=PROJECT_NAME)\n",
    "update_with_uid(SENTIMENT_DF, PROJECT_NAME, DATASET_NAME)\n",
    "assert dataset.data_row_for_external_id(str(SENTIMENT_DF.head(1).id.to_numpy()[0]))\n",
    "maybe_create_ontology(PROJECT_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a model\n",
    "Thanks to advances in deep learning for NLP, we do not have train a language model from scratch.  We use a [distilled](https://en.wikipedia.org/wiki/Knowledge_distillation) version of [BERT](https://github.com/google-research/bert): [DistilBERT](https://arxiv.org/abs/1910.01108) to generate embeddings of our tweets.  We then pool these embeddings together and treat them as features for a logistic regression to learn sentiment.\n",
    "\n",
    "Note: Depending on the hardware this is run on, embedding 8000 tweets may take 15-20 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collapse\n",
    "model = DistilBertModel.from_pretrained(\"distilbert-base-uncased\", cache_dir='model_dir')\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\", cache_dir='model_dir')\n",
    "\n",
    "\n",
    "def encode_text(text):\n",
    "    '''Embed text with DistilBERT and return average pool.\n",
    "    \n",
    "    Note: YMMV with average pool vs max pool.  average pool was just chosen for simplicity.\n",
    "    '''\n",
    "    input_ids = torch.tensor([tokenizer.encode(text)])\n",
    "    return model(input_ids)[0].mean(1)[0].detach().numpy()\n",
    "\n",
    "\n",
    "def add_encoded(df: pd.core.frame.DataFrame) -> pd.core.frame.DataFrame:\n",
    "    '''Add features encoded by DistilBERT and an average pool as a column.'''\n",
    "    df['encoded_text'] = df.text.apply(encode_text)\n",
    "    return df\n",
    "\n",
    "\n",
    "def train_model(train: pd.core.frame.DataFrame) -> LogisticRegression:\n",
    "    '''Trains logistic regression model over given training data.'''\n",
    "    model = LogisticRegression(solver=\"liblinear\")\n",
    "    model.fit(list(train.encoded_text.values), list(train.sentiment.values))\n",
    "    return model\n",
    "\n",
    "\n",
    "def eval_model(model: LogisticRegression, test: pd.core.frame.DataFrame) -> float:\n",
    "    '''Calculates AUC of the ROC of the trained model on given test data.\n",
    "    \n",
    "    https://en.wikipedia.org/wiki/Receiver_operating_characteristic#Area_under_the_curve\n",
    "    '''\n",
    "    # auc score entire over test split\n",
    "    roc = roc_auc_score(\n",
    "        list(test.sentiment.values),\n",
    "        model.predict_proba(list(test.encoded_text.values))[:,1])\n",
    "    return roc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "SENTIMENT_DF = add_encoded(SENTIMENT_DF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = SENTIMENT_DF[SENTIMENT_DF.split == 'train']\n",
    "test = SENTIMENT_DF[SENTIMENT_DF.split == 'test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "We begin our first iteration by prioritizing any 100 datarows for human annotation **via [Queue customization](https://labelbox.com/docs/api/queue-customization#setting_queue_prioritization) in Labelbox**.\n",
    "\n",
    "1. Set priority of 100 datarows to `1`\n",
    "2. Set priority of the remainder datarows to `2`\n",
    "\n",
    "Note: Priority for all datarows should be set, [otherwise the queue may default to lexicographical resolution of priority](https://labelbox.com/docs/api/queue-customization#prioritization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#collapse\n",
    "def fetch_labeling_parameter_overrides(project_name: str) -> List:\n",
    "    '''Fetches all datarows with labeling parameter overrides in a project.'''\n",
    "    project_id = maybe_create_project(project_name).uid\n",
    "    client = graphql_client()\n",
    "    \n",
    "    overrides = []\n",
    "    override = True\n",
    "    while override:\n",
    "        response = client.execute(\n",
    "            f'''\n",
    "            query pullLabelingParameters{{\n",
    "              project(where: {{id: \"{project_id}\"}}) {{\n",
    "                labelingParameterOverrides{{\n",
    "                  id\n",
    "                  priority\n",
    "                }}\n",
    "              }}\n",
    "            }}\n",
    "            '''\n",
    "        )\n",
    "        override = json.loads(response)['data']['project']['labelingParameterOverrides']\n",
    "        overrides.extend(override)\n",
    "    return overrides\n",
    "    \n",
    "\n",
    "def prioritize(project_name: str, dataset_name: str, uids: List[str]) -> None:\n",
    "    '''Prioritizes given uids for labeling and deprioritizes all other.\n",
    "    \n",
    "    NOTE: We have to deprioritize so that we don't rely on queue rebuilding to\n",
    "    avoid default lexicographical sorting.\n",
    "    > When setting queue prioritization on an active queue, and without the queue rebuilding,\n",
    "      the given data rows will have their priority updated, and if thereâ€™s a collision\n",
    "      (as in above, where two data rows have a priority of 1), they will be ordered lexicographically.\n",
    "      \n",
    "    https://labelbox.com/docs/api/queue-customization#setting_queue_prioritization\n",
    "    \n",
    "    '''\n",
    "    project_id = maybe_create_project(project_name).uid\n",
    "    dataset = maybe_create_sentiment_dataset(\n",
    "        dataset_name=dataset_name, project_name=project_name)\n",
    "    \n",
    "    priority_data_rows = (\n",
    "        f'{{dataRow: {{id: \"{uid}\"}}, priority: 1, numLabels: 1}}'\n",
    "        for uid in uids\n",
    "    )\n",
    "    rest_data_rows = (\n",
    "        f'{{dataRow: {{id: \"{data_row.uid}\"}}, priority: 2, numLabels: 1}}'\n",
    "        for data_row in dataset.data_rows()\n",
    "        if data_row.uid not in uids\n",
    "    )\n",
    "    data_rows = chain(priority_data_rows, rest_data_rows)\n",
    "    \n",
    "    client = graphql_client()\n",
    "    \n",
    "    def batches(iterable, size):\n",
    "        iterator = iter(iterable)\n",
    "        for first in iterator:\n",
    "            yield chain([first], islice(iterator, size - 1))\n",
    "\n",
    "    for batch in batches(data_rows, size=999):\n",
    "        response = client.execute(\n",
    "            f'''\n",
    "            mutation setLabelingParameterOverrides {{\n",
    "              project(where: {{ id: \"{project_id}\" }}) {{\n",
    "                setLabelingParameterOverrides(data: [\n",
    "                    {','.join(batch)}\n",
    "                ]) {{\n",
    "                  success\n",
    "                }}\n",
    "              }}\n",
    "            }}\n",
    "            '''\n",
    "        )\n",
    "        assert not json.loads(response).get('errors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "prioritize(PROJECT_NAME, DATASET_NAME, SENTIMENT_DF.head(100).uid.to_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When these datarows are labeled, we would export them via [bulk export](https://labelbox.com/docs/python-api/labels#export_labels) or [webhooks](https://labelbox.com/docs/api/webhooks).  However, in this example, we can just take the first 100 from the already labeled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC: 0.728004201286644\n"
     ]
    }
   ],
   "source": [
    "annotation_budget = 100\n",
    "labeled_training_data = train[:annotation_budget]\n",
    "model = train_model(labeled_training_data)\n",
    "roc = eval_model(model, test)\n",
    "print(f'ROC: {roc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uncertainty sampling\n",
    "\n",
    "We will employ a fairly straightforward definition of model uncertainty: one minus the largest posterior probability of our model.  The intuition here is: given the the highest posterior probability is the model's prediction, we take the remaining probability left until the model would have been 100% certain as the uncertainty.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uncertainty_score(\n",
    "        rows: pd.core.frame.DataFrame,\n",
    "        model: LogisticRegression) -> float:\n",
    "    '''Returns uncertainty of the model on given data.'''\n",
    "    return 1 - model.predict_proba(list(rows.encoded_text)).max(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the subsequent batches of annotations, we will pick top 100 datarows for which the model is the most uncertain.  For demonstration's sake, we work with our already labeled dataset.  In reality, one would continue employing the above outlined prioritization via [Queue customization](https://labelbox.com/docs/api/queue-customization#setting_queue_prioritization) in Labelbox."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collapse\n",
    "def naive_sampler(\n",
    "        sample: pd.core.frame.DataFrame,\n",
    "        train: pd.core.frame.DataFrame,\n",
    "        num: int, _) -> pd.core.frame.DataFrame:\n",
    "    '''Returns the next number of datarows that have not been trained on.'''\n",
    "    untrained = train[~train.index.isin(sample.index)]\n",
    "    return pd.concat([sample, untrained.head(num)])\n",
    "\n",
    "def uncertainty_sampler(\n",
    "        sample: pd.core.frame.DataFrame,\n",
    "        train: pd.core.frame.DataFrame,\n",
    "        num: int,\n",
    "        model: LogisticRegression) -> pd.core.frame.DataFrame:\n",
    "    '''Returns the top number of most uncertain datarows.'''\n",
    "    \n",
    "    untrained = train[~train.index.isin(sample.index)]\n",
    "    def uncertainty_sampling(\n",
    "            rows: pd.core.frame.DataFrame,\n",
    "            num: int,\n",
    "            model: LogisticRegression) -> pd.core.frame.DataFrame:\n",
    "        temp_df = rows.copy()\n",
    "        temp_df['uncertainty'] = uncertainty_score(rows, model)\n",
    "        temp_df = temp_df.sort_values('uncertainty', ascending=False)\n",
    "        return temp_df[:num]\n",
    "\n",
    "    return pd.concat([sample, uncertainty_sampling(untrained, num, model)])\n",
    "\n",
    "\n",
    "def batch_roc(\n",
    "        train: pd.core.frame.DataFrame,\n",
    "        batches: List[int],\n",
    "        sampling_method: str) -> List[float]:\n",
    "    '''Returns ROC metric for every addition to the training data.\n",
    "    \n",
    "    The sampling_method will dictate how new data is selected\n",
    "    for labeling and inclusion into training data.\n",
    "\n",
    "    '''\n",
    "    SAMPLING_METHODS = {\n",
    "        'naive': naive_sampler,\n",
    "        'uncertainty': uncertainty_sampler\n",
    "    }\n",
    "\n",
    "    rocs = []\n",
    "    batch_size = batches[1] - batches[0]\n",
    "    sample = train[:batch_size]\n",
    "    \n",
    "    for _ in batches:\n",
    "        model = train_model(sample)\n",
    "        roc = eval_model(model, test)\n",
    "        #roc = eval_model(model, train)\n",
    "        rocs.append(roc)\n",
    "        sample = SAMPLING_METHODS[sampling_method](sample, train, batch_size, model)\n",
    "    return rocs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "We compare uncertainty sampling against naive sampling where random data is selected at every iteration until all training data is labeled and trained over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-0c9c209b92fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#collapse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mbatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnaive_rocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_roc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'naive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "#collapse\n",
    "batch_size = 100\n",
    "batches = np.arange(100, len(train), batch_size)\n",
    "\n",
    "naive_rocs = batch_roc(train, batches, 'naive')\n",
    "uncertainty_rocs = batch_roc(train, batches, 'uncertainty')\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "lines = [\n",
    "    ax.plot(batches, uncertainty_rocs, '-', color='blue', label='uncertainty sampling'),\n",
    "    ax.plot(batches, naive_rocs, '--', color='black', label='random sampling'),\n",
    "]\n",
    "ax.set_xlabel('# of labeled data in training set')\n",
    "ax.set_ylabel('ROC on test set')\n",
    "plt.legend()\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/roc_uncertainty_random_7200_800.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model trained with training examples chosen by uncertainty sampling was able to reach asymptotic performance by around 3000 annotations.  The model trained by naive random sampling requires a bit over 5000 examples to perform equally well.  Not only does this minimize the number of assets that require human annotation, it minimizes the number of examples the model has to be trained with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "[Queue customization](https://labelbox.com/docs/api/queue-customization#setting_queue_prioritization) in Labelbox is an incredibly simple way to start using active learning.  By prioritizing certain datarows for annotation, machine learning systems can achieve their goals with less annotation cost and fewer training cycles.  In many cases, the datarows that a model is uncertain of are also often more engaging to annotate and more important to review, allowing the human components of your machine learning system to spend their time on high impact decisions.\n",
    "\n",
    "\n",
    "## Practical considerations\n",
    "\n",
    "Modern neural networks are deeper and datasets are larger, retraining may not be ideal or necessary.  Active learning is also a useful tool for transfer learning or finetuning frameworks.  Problems may also not be classifications but segmentation.  We encourage you to seek domain specific literature for inspiration.\n",
    "\n",
    "For example, there are other ways of measuring uncertainty, such as classification entropy or classification margin.  Or your task might be include both [epistemic and aleatoric uncertainty](https://arxiv.org/abs/1909.00218).  In fact, **uncertainty sampling** is far from the only way of surfacing informative examples.\n",
    "\n",
    "For ensemble-based approaches, disagreement sampling may be more useful, surfacing examples where the ensemble is highly disagreeable.\n",
    "\n",
    "For datasets with a long tail of rare examples, diversity sampling proposes representative examples for prioritization.\n",
    "\n",
    "\n",
    "## Next Steps\n",
    "The example covered here uses uncertainty sampling to address a binary sentiment classification problem.  What are some of your use cases?  How can Labelbox help:\n",
    "\n",
    "1. Surface informative datarows\n",
    "1. Explore datarows on the decision boundary\n",
    "\n",
    "We would love to hear more about them!\n",
    "\n",
    "\n",
    "## Additional Reading\n",
    "1. [Settles, Burr.  *Active Learning Literature Survey*.](http://burrsettles.com/pub/settles.activelearning.pdf)\n",
    "1. [modAL: A modular active learning framework for Python3.](https://modal-python.readthedocs.io/en/latest/index.html#)\n",
    "1. [Building the Software 2.0 Stack (Andrej Karpathy)](https://www.youtube.com/watch?v=y57wwucbXR8)\n",
    "1. [Deep Learning: State of the Art (2020)](https://lexfridman.com/files/slides/2020_01_06_deep_learning_state_of_the_art.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
